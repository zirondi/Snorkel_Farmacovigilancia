{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<sqlalchemy.orm.session.Session object at 0x7f9e9962acf8>\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.system(\"ln -s /var/run/postgresql/.s.PGSQL.5432 /tmp/.s.PGSQL.5432\")\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Clearing existing...\n  0%|          | 0/5982 [00:00<?, ?it/s]Running UDF...\n100%|██████████| 5982/5982 [01:17<00:00, 77.11it/s]CPU times: user 2.28 s, sys: 830 ms, total: 3.11 s\nWall time: 1min 20s\nDocuments: 5982\nSentences: 6035\n\n\n"
    }
   ],
   "source": [
    "#Numero total de artigos no articles.tsv\n",
    "n_docs = 5982\n",
    "\n",
    "from snorkel.parser import TSVDocPreprocessor\n",
    "doc_preprocessor = TSVDocPreprocessor('/home/lzirondi/datasets/Processed/Tweets Anotados.tsv', max_docs=n_docs)\n",
    "\n",
    "#\n",
    "\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs, parallelism=16)\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0%|          | 0/4828 [00:00<?, ?it/s]CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\nWall time: 9.78 µs\nClearing existing...\nRunning UDF...\n100%|██████████| 4828/4828 [03:25<00:00, 23.45it/s]\n  0%|          | 0/602 [00:00<?, ?it/s]Number of candidates: 14395\nClearing existing...\nRunning UDF...\n100%|██████████| 602/602 [00:15<00:00, 28.40it/s]\n  0%|          | 0/605 [00:00<?, ?it/s]Number of candidates: 1739\nClearing existing...\nRunning UDF...\n100%|██████████| 605/605 [00:15<00:00, 31.85it/s]Number of candidates: 1825\n\n"
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "causas = candidate_subclass('causas', ['Substância', 'Evento_Adverso'])\n",
    "\n",
    "#\n",
    "def subsToList():\n",
    "\n",
    "    f = open('/home/lzirondi/datasets/Processed/Dicionario_de_Substancias.txt', 'r', encoding='UTF-8')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    s = s.splitlines()\n",
    "\n",
    "    return s\n",
    "\n",
    "def eventosToList():\n",
    "    f = open('/home/lzirondi/datasets/Processed/Dicionario_de_Eventos.txt', 'r', encoding='UTF-8')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    s = s.splitlines()\n",
    "\n",
    "    return s\n",
    "\n",
    "substancias = subsToList()\n",
    "eventos = eventosToList()\n",
    "\n",
    "\n",
    "\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import DictionaryMatch\n",
    "\n",
    "ngrams = Ngrams(n_max=7)\n",
    "substancia_matcher = DictionaryMatch(d=substancias, longest_match_only=True, ignore_case=True)\n",
    "eventos_matcher = DictionaryMatch(d=eventos, longest_match_only=True, ignore_case=True)\n",
    "cand_extractor = CandidateExtractor(causas, [ngrams, ngrams], [substancia_matcher, eventos_matcher])\n",
    "\n",
    "from snorkel.models import Document\n",
    "from util import number_of_people\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 10 == 8:\n",
    "            dev_sents.add(s)\n",
    "        elif i % 10 == 9:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            train_sents.add(s)\n",
    "                \n",
    "#\n",
    "\n",
    "%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i, parallelism = 16)\n",
    "    print(\"Number of candidates:\", session.query(causas).filter(causas.split == i).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = session.query(causas).filter(causas.split == 0).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3929::span:43:50\n<class 'str'>\n['3929', '', 'span', '43', '50']\n"
    }
   ],
   "source": [
    "t = cands[40]\n",
    "a = t.get_contexts()\n",
    "#print(t)\n",
    "#print(a)\n",
    "span1 = a[0]\n",
    "id = span1.get_stable_id()\n",
    "print(id)\n",
    "print(type(id))\n",
    "s = id.split(':')\n",
    "print(s)\n",
    "#0id, 1nada, 2span, 3charinicial, 4final\n",
    "\n",
    "\n",
    "#print(dir(t))\n",
    "#print(t.get_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "causas(Span(\"b'nimesulida'\", sentence=68448, chars=[25,34], words=[6,6]), Span(\"b'dor de garganta'\", sentence=68448, chars=[76,90], words=[16,18]))\n2719\t ja tomei dois comprimidoS DE NIMEsulida e essa dor nao passa .\tnimesulida\tdor de garganta\n\ncausas(Span(\"b'e'\", sentence=68448, chars=[47,47], words=[9,9]), Span(\"b'dor de garganta'\", sentence=68448, chars=[76,90], words=[16,18]))\n2719\t ja tomei dois comprimidos de nimesulida e essa dor nao passa .\te\tdor de garganta\n\ncausas(Span(\"b'e'\", sentence=68448, chars=[36,36], words=[7,7]), Span(\"b'dor de garganta'\", sentence=68448, chars=[76,90], words=[16,18]))\n2719\t ja tomei dois comprimidos de nimesulida e essa dor nao passa .\te\tdor de garganta\n\ncausas(Span(\"b'e'\", sentence=68448, chars=[102,102], words=[21,21]), Span(\"b'dor de garganta'\", sentence=68448, chars=[76,90], words=[16,18]))\n2719\t ja tomei dois comprimidos de nimesulida e essa dor nao passa .\te\tdor de garganta\n\ncausas(Span(\"b'a'\", sentence=67061, chars=[50,50], words=[10,10]), Span(\"b'dor de cabeca'\", sentence=67061, chars=[52,64], words=[11,13]))\n1368\t dai nao sei se e alergia do protetor solar novo ou DE NIMESULIDa .\ta\tdor de cabeca\n\n"
    }
   ],
   "source": [
    "#0id, 1nada, 2span, 3charinicial, 4final\n",
    "\n",
    "query = session.query(Sentence)\n",
    "\n",
    "tsv = open('candidatos.tsv', 'w', encoding='UTF-8')\n",
    "\n",
    "#for candidate in session.query(causas).filter(causas.split == 0).all():\n",
    "#    context = candidate.get_contexts()\n",
    "\n",
    "candidate = session.query(causas).filter(causas.split == 0).all()\n",
    "\n",
    "for i in range(13,18):\n",
    "    context = candidate[i]\n",
    "    print(context)\n",
    "    span1 = context[0]\n",
    "    span2 = context[1]\n",
    "\n",
    "    span1_text = str(context[0]).split('\\\"')[1].split('\\'')[1] \n",
    "    span2_text = str(context[1]).split('\\\"')[1].split('\\'')[1]\n",
    "\n",
    "    id1 = span1.get_stable_id().split(':')\n",
    "    id2 = span2.get_stable_id().split(':')\n",
    "\n",
    "    sentence = query[int(id1[0])]\n",
    "\n",
    "    asdict = sentence._asdict()\n",
    "\n",
    "    document = str(asdict['document']).split(' ')[1]\n",
    "    \n",
    "    text = asdict['text']\n",
    "    n1 = int(id1[3]) \n",
    "    m1 = int(id1[4])\n",
    "    cand_1 = text[n1:m1]\n",
    "    \n",
    "    n2 = int(id2[3])\n",
    "    m2 = int(id2[4])\n",
    "    cand_2 = text[n2:m2]\n",
    "\n",
    "    text = text.replace(cand_1, cand_1.upper())\n",
    "    text = text.replace(cand_2, cand_2.upper())\n",
    "    text = text.replace('\\n', '')\n",
    "\n",
    "    line = document + '\\t' + text + '\\t' + span1_text + '\\t' + span2_text + '\\n'\n",
    "    print(line)\n",
    "\n",
    "    tsv.write(line)\n",
    "\n",
    "\n",
    "tsv.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Span(', \"b'dipirona'\", ', sentence=70853, chars=[7,14], words=[2,2])']\n['b', 'dipirona', '']\ndipirona\n"
    }
   ],
   "source": [
    "cands = session.query(causas).filter(causas.split == 0).all()\n",
    "context = cands[0]\n",
    "spn1 = context[0]\n",
    "spn2 = context[1]\n",
    "\n",
    "s = str(spn1).split(\"\\\"\")\n",
    "print(s)\n",
    "s2 = s[1].split('\\'')\n",
    "print(s2)\n",
    "print(s2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Snorkel7)",
   "language": "python",
   "name": "snorkel7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}