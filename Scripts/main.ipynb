{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reexecução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "relation = candidate_subclass('Causa', ['Substância', 'Evento_Adverso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths & Magic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "inPath = '/home/lzirondi/Github/AlexandreMartins/'\n",
    "outPath = '/home/lzirondi/Github/snorkel/Scripts/Datasets'\n",
    "corpusPath = outPath + '/Processed/Tweets Anotados.tsv'\n",
    "LIWCpath = '/home/lzirondi/Github/snorkel/Scripts/'\n",
    "\n",
    "\n",
    "\n",
    "#Calculando o número de linhas no corpus\n",
    "with open(corpusPath) as tsv:\n",
    "    s = tsv.readlines()\n",
    "    n_docs = len(s)\n",
    "    s = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nova execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo o link simbólico para o acesso ao banco de dados e definindo o banco como o PSQL\n",
    "import os\n",
    "os.system(\"ln -s /var/run/postgresql/.s.PGSQL.5432 /tmp/.s.PGSQL.5432\")\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "\n",
    "#O import do SnorkelSession SEMPRE vai ter que estar após definir o BD nas variaveis de ambiente do SO, ao contrário ele defaulta para SQLite\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5984 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5984/5984 [01:25<00:00, 69.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.97 s, sys: 824 ms, total: 3.79 s\n",
      "Wall time: 1min 27s\n",
      "Documents: 5984\n",
      "Sentences: 6031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import TSVDocPreprocessor\n",
    "doc_preprocessor = TSVDocPreprocessor(corpusPath, max_docs=n_docs)\n",
    "\n",
    "#\n",
    "\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs, parallelism=16)\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Tweets Anotados.txt copiado com sucesso.\n",
      "Arquivo Eventos.txt copiado com sucesso.\n",
      "Arquivo Substâncias2-br-gazette.txt copiado com sucesso.\n",
      "Arquivo Remédios2-br-gazette.txt copiado com sucesso.\n",
      "Arquivo Substâncias-br-gazette.txt copiado com sucesso.\n",
      "Arquivo Remédios-br-gazette.txt copiado com sucesso.\n",
      "Arquivo EventosAdversos-gazette.txt copiado com sucesso.\n",
      "Tsv do Corpus criado com sucesso.\n",
      "Dicionário de Substâncias gerado com sucesso.\n",
      "Dicionário de Eventos gerado com sucesso.\n",
      "Dicionário LIWC de Causas gerado com sucesso.\n",
      "Dicionário LIWC de Verbos gerado com sucesso.\n",
      "Dicionário LIWC de Causas & Verbos gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import Preprocessing as prep\n",
    "\n",
    "\n",
    "util = prep.Util(inPath, outPath, LIWCpath, True)\n",
    "\n",
    "\n",
    "substancias = util.subsToList()\n",
    "eventos = util.eventosToList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando a Classe de Candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "relation = candidate_subclass('Causa', ['Substância', 'Evento_Adverso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo Candidatos do Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4825 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 11 µs\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4825/4825 [02:05<00:00, 38.29it/s]\n",
      "  0%|          | 0/603 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 6398\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 603/603 [00:10<00:00, 56.82it/s]\n",
      "  0%|          | 0/603 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 765\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 603/603 [00:13<00:00, 45.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import DictionaryMatch\n",
    "\n",
    "ngrams = Ngrams(n_max=7)\n",
    "substancia_matcher = DictionaryMatch(d=substancias, longest_match_only=True, ignore_case=True)\n",
    "eventos_matcher = DictionaryMatch(d=eventos, longest_match_only=True, ignore_case=True)\n",
    "cand_extractor = CandidateExtractor(relation, [ngrams, ngrams], [substancia_matcher, eventos_matcher], symmetric_relations=True)\n",
    "\n",
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 10 == 8:\n",
    "            dev_sents.add(s)\n",
    "        elif i % 10 == 9:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            train_sents.add(s)\n",
    "                \n",
    "#\n",
    "\n",
    "%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i, parallelism = 16)\n",
    "    print(\"Number of candidates:\", session.query(relation).filter(relation.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução Incompleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando um .tsv dos candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "f = open('/home/lzirondi/Github/snorkel/Scripts/Datasets/Processed/Candidatos.tsv', 'w', encoding='UTF-8')\n",
    "f.write('Sentença\\t Span_1 \\t Span_2 \\n')\n",
    "\n",
    "for candidate in cands:   \n",
    "    \n",
    "    parent = candidate.get_parent()\n",
    "    parent = parent._asdict()\n",
    "    parent = parent.get('text')\n",
    "    parent = parent.replace('\\n', '')\n",
    "    \n",
    "    \n",
    "    contexts = candidate.get_contexts()\n",
    "    span_list = [contexts[0].get_span(), contexts[1].get_span()]   \n",
    "    \n",
    "    \n",
    "    f.write(parent + '\\t' + span_list[0] + '\\t' + span_list[1] + '\\n' )\n",
    "    \n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#par = candi.get_parent()\n",
    "#dict1 = par._asdict()\n",
    "\n",
    "#contexts = candi.get_contexts()\n",
    "\n",
    "#spans = [contexts[0], contexts[1]]\n",
    "\n",
    "#a = contexts[0]\n",
    "\n",
    "#for candidate in cands:\n",
    "#    print(candidate.get_contexts()[0].get_span())\n",
    "#print(a.get_span())\n",
    "\n",
    "\n",
    "#object_methods = [method_name for method_name in dir(a) if callable(getattr(a, method_name))]\n",
    "\n",
    "#print(object_methods)\n",
    "#cands = session.query(causas).filter(causas.split == 0).all()\n",
    "\n",
    "#cands2 = session.query(causas).filter(causas.split==0).all()\n",
    "relation = candidate_subclass('Causa', ['Substância', 'Evento_Adverso'])\n",
    "\n",
    "print(session.query(causas).filter(causas.split==0).all())\n",
    "\n",
    "#print(cands)\n",
    "#print(cands[527])\n",
    "\n",
    "#print(cands2[527])\n",
    "\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(parent))\n",
    "#print(type(contexts_list[0]))\n",
    "#print(type(contexts_list[0]))\n",
    "#print(type(cands[0]))\n",
    "    \n",
    "\n",
    "#print(cands[0].get_cids())\n",
    "#print(type(cands[0].get_contexts()))\n",
    "#print(cands[0].get_parent())\n",
    "\n",
    "#object_methods = [method_name for method_name in dir(cands[0]) if callable(getattr(cands[0], method_name))]\n",
    "\n",
    "#print(object_methods)\n",
    "\n",
    "#a = cands[0].get_contexts()\n",
    "\n",
    "#object_methods = [method_name for method_name in dir(contexts[0]) if callable(getattr(contexts[0], method_name))]\n",
    "\n",
    "#print(object_methods)\n",
    "\n",
    "#print(type(a))\n",
    "\n",
    "#f = open('/home/lzirondi/Github/snorkel/Scripts/Datasets/Processed/Candidatos.txt', 'w', encoding='UTF-8')\n",
    "#for span in cands:\n",
    "#    print(str(span))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liwcInCandidate(c):\n",
    "    return 1 if se.contains(c.get_parent().words) else 0\n",
    "\n",
    "    \n",
    "\n",
    "for c in session.query(relation).filter(relation.split == 1).all():\n",
    "    if liwcInCandidate(c) != 0:\n",
    "        labeled.append(c)\n",
    "print(\"Number labeled:\", len(labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "\n",
    "#SentenceNgramViewer(labeled, session)\n",
    "\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "print(cands[0])\n",
    "SentenceNgramViewer(cands, session)\n",
    "\n",
    "#c = cands[0]\n",
    "\n",
    "#print(c.get_parent().words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Snorkel7)",
   "language": "python",
   "name": "snorkel7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
