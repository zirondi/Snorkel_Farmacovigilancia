{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "snorkel-extraction",
   "display_name": "snorkel-extraction",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# **THIS SHOULD BE RUN ON SNORKEL-EXTRACTION**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Pre-reqs:\n",
    "\n",
    "1. https://github.com/snorkel-team/snorkel-extraction shoud be installed and *working*\n",
    "2. https://github.com/gpgs1978/AlexandreMartins should be git-cloned on '~'\n",
    "3. Preprocessing.py (link to it in the future) should be in the same folder as this notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Paths, magic functions and needed vars and dicts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Arquivo Tweets Anotados.txt copiado com sucesso.\nArquivo Eventos.txt copiado com sucesso.\nArquivo Substâncias2-br-gazette.txt copiado com sucesso.\nArquivo Remédios2-br-gazette.txt copiado com sucesso.\nArquivo Substâncias-br-gazette.txt copiado com sucesso.\nArquivo Remédios-br-gazette.txt copiado com sucesso.\nArquivo EventosAdversos-gazette.txt copiado com sucesso.\nTsv do Corpus criado com sucesso.\nDicionário de Substâncias gerado com sucesso.\nDicionário de Eventos gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "inPath = os.path.expanduser('~') + '/AlexandreMartins'\n",
    "outPath = os.path.expanduser('~') \n",
    "\n",
    "import Preprocessing as prep\n",
    "\n",
    "util = prep.Util(inPath, outPath, True)\n",
    "\n",
    "substancias = util.subsToList()\n",
    "eventos = util.eventosToList()\n",
    "\n",
    "#Calculando o número de linhas no corpus\n",
    "corpusPath = os.path.expanduser('~') + '/Processed/Tweets Anotados.tsv'\n",
    "\n",
    "with open(corpusPath) as tsv:\n",
    "    s = tsv.readlines()\n",
    "    n_docs = len(s)\n",
    "    s = None"
   ]
  },
  {
   "source": [
    "## Conexão ao banco"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fazendo o link simbólico para o acesso ao banco de dados e definindo o banco como o PSQL\n",
    "os.system(\"ln -s /var/run/postgresql/.s.PGSQL.5432 /tmp/.s.PGSQL.5432\")\n",
    "\n",
    "#THIS SHOULD CHANGE IN THE FUTURE\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "\n",
    "#O import do SnorkelSession SEMPRE vai ter que estar após definir o BD nas variaveis de ambiente do SO, ao contrário ele defaulta para SQLite\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "source": [
    "## Gerando Sentenças"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/5984 [00:00<?, ?it/s]Clearing existing...\n",
      "Running UDF...\n",
      "100%|██████████| 5984/5984 [06:19<00:00, 15.79it/s]CPU times: user 7.94 s, sys: 1.58 s, total: 9.51 s\n",
      "Wall time: 6min 19s\n",
      "Documents: 5984\n",
      "Sentences: 8280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import TSVDocPreprocessor\n",
    "\n",
    "doc_preprocessor = TSVDocPreprocessor(corpusPath, max_docs=n_docs)\n",
    "\n",
    "\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy(lang='pt'))\n",
    "\n",
    "\n",
    "%time corpus_parser.apply(doc_preprocessor, count=n_docs, parallelism=16)\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "source": [
    "## Gerando a Classe de Relação"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "relation = candidate_subclass('Relation', ['Substância', 'Evento_Adverso'])"
   ]
  },
  {
   "source": [
    "## Extraindo Candidatos do Corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import DictionaryMatch\n",
    "\n",
    "ngrams = Ngrams(n_max=7)\n",
    "substancia_matcher = DictionaryMatch(d=substancias, longest_match_only=True, ignore_case=True)\n",
    "eventos_matcher = DictionaryMatch(d=eventos, longest_match_only=True, ignore_case=True)\n",
    "cand_extractor = CandidateExtractor(relation, [ngrams, ngrams], [substancia_matcher, eventos_matcher], symmetric_relations=True)\n",
    "\n",
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 10 == 8:\n",
    "            dev_sents.add(s)\n",
    "        elif i % 10 == 9:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            train_sents.add(s)"
   ]
  },
  {
   "source": [
    "## Aplicando o extrator de relação"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/6659 [00:00<?, ?it/s]CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.68 µs\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "100%|██████████| 6659/6659 [01:59<00:00, 55.72it/s]\n",
      "  0%|          | 0/810 [00:00<?, ?it/s]Number of candidates: 5156\n",
      "1\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "100%|██████████| 810/810 [00:12<00:00, 66.36it/s]\n",
      "  0%|          | 0/811 [00:00<?, ?it/s]Number of candidates: 652\n",
      "2\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "100%|██████████| 811/811 [00:12<00:00, 63.17it/s]Number of candidates: 626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    print(i)\n",
    "    cand_extractor.apply(sents, split=i, parallelism = 16)\n",
    "    print(\"Number of candidates:\", session.query(relation).filter(relation.split == i).count())"
   ]
  },
  {
   "source": [
    "## Definindo Helper Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import is_inverted\n",
    "\n",
    "def get_text(cand):\n",
    "    return cand.get_parent()._asdict().get('text')\n",
    "\n",
    "def get_spans_all(cand):\n",
    "    return [ \n",
    "                [\n",
    "                    cand.get_contexts()[0].get_span(), \n",
    "                    str(cand.get_contexts()[0].char_start), \n",
    "                    str(cand.get_contexts()[0].char_end), \n",
    "                    cand.get_contexts()[0].__str__().split(' ')[3][9:-1].replace(']', '').replace(',', '')\n",
    "                ],\n",
    "                [\n",
    "                    cand.get_contexts()[1].get_span(), \n",
    "                    str(cand.get_contexts()[1].char_start), \n",
    "                    str(cand.get_contexts()[1].char_end), \n",
    "                    cand.get_contexts()[1].__str__().split(' ')[3][9:-1].replace(']', '').replace(',', '')\n",
    "                ]\n",
    "            ]"
   ]
  },
  {
   "source": [
    "## Gerando os tsvs para a execução do snorkel9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 18/5156 [00:00<00:29, 174.34it/s]\n",
      " train_sets.tsv\n",
      "100%|██████████| 5156/5156 [00:25<00:00, 200.90it/s]\n",
      "  3%|▎         | 20/652 [00:00<00:03, 199.27it/s]Done  train_sets.tsv\n",
      "\n",
      " dev_sets.tsv\n",
      "100%|██████████| 652/652 [00:03<00:00, 202.23it/s]\n",
      "  4%|▎         | 22/626 [00:00<00:02, 211.71it/s]Done  dev_sets.tsv\n",
      "\n",
      " test_sets.tsv\n",
      "100%|██████████| 626/626 [00:03<00:00, 203.74it/s]Done  test_sets.tsv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ajustar output folder\n",
    "from tqdm import tqdm\n",
    "names = ['train_sets.tsv', 'dev_sets.tsv', 'test_sets.tsv']\n",
    "\n",
    "columns = \"Candidate\\tEvent\\tE_Char_Start\\tE_Char_end\\tE_Word_Index\\tRemedy\\tR_Char_Start\\tR_Char_end\\tR_Word_Index\\tIs_inverted\\tLabel\\n\"\n",
    "\n",
    "label = '-1'\n",
    "\n",
    "for i in range(3):\n",
    "    with open(names[i], 'w') as f:\n",
    "        print('\\n', names[i])\n",
    "        f.write(columns)\n",
    "        for c in tqdm(session.query(relation).filter(relation.split == i).all()):\n",
    "            \n",
    "            candidate = get_text(c).replace('\\n', '')\n",
    "            remedy, event = get_spans_all(c)\n",
    "            inverted = '1' if is_inverted(c) else '0'\n",
    "\n",
    "            f.write(\n",
    "                candidate + '\\t' +  \n",
    "                remedy[0] + '\\t' + \n",
    "                remedy[1] + '\\t' + \n",
    "                remedy[2] + '\\t' + \n",
    "                remedy[3] + '\\t' +  \n",
    "                event[0] + '\\t' + \n",
    "                event[1] + '\\t' + \n",
    "                event[2] + '\\t' + \n",
    "                event[3] + '\\t' +\n",
    "                inverted + '\\t' +\n",
    "                label + '\\n'\n",
    "            )\n",
    "        print('Done ', names[i])\n",
    "            "
   ]
  }
 ]
}