{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THIS MUST BE RUN ON SNORKEL-EXTRACTION**\n",
    "## Pre-reqs:\n",
    "\n",
    "1. https://github.com/snorkel-team/snorkel-extraction shoud be installed and *working*\n",
    "2. https://github.com/gpgs1978/AlexandreMartins should be git-cloned on '~'\n",
    "3. https://github.com/zirondi/Snorkel_Farmacovigilancia should be git-cloned on '~'\n",
    "4. [Preprocessing.py](https://github.com/zirondi/Snorkel_Farmacovigilancia/blob/master/Scripts/Text-Preprocessing/Preprocessing.py) (link to it in the future) should be in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths, magic functions and needed vars and dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Tweets Anotados.txt was successfully copied\n",
      "File Remédios2-br-gazette.txt was successfully copied\n",
      "File EventosAdversos-gazette.txt was successfully copied\n",
      "File Substâncias-br-gazette.txt was successfully copied\n",
      "File Eventos.txt was successfully copied\n",
      "File Remédios-br-gazette.txt was successfully copied\n",
      "File Substâncias2-br-gazette.txt was successfully copied\n",
      ".tsv Corpus created successfully.\n",
      ".txt Drugs Dictionary created successfully.\n",
      ".txt Events Dictionary created successfully.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "separator = os.path.sep\n",
    "inPath = os.path.expanduser('~') + separator + 'AlexandreMartins'\n",
    "outPath = os.path.abspath('').split('Scripts' + separator + 'Text-Preprocessing')[0] + 'Datasets'\n",
    "\n",
    "import Preprocessing as prep\n",
    "\n",
    "util = prep.Util(inPath, outPath + separator + 'Preprocessed' + separator + 'Source')\n",
    "\n",
    "drugs = util.drugs_to_list()\n",
    "events = util.events_to_list()\n",
    "\n",
    "#Calculating the number of lines in the corpus\n",
    "corpusPath = os.path.expanduser('~') + f'{separator}Processed{separator}Tweets Anotados.tsv'\n",
    "\n",
    "with open(corpusPath) as tsv:\n",
    "    s = tsv.readlines()\n",
    "    n_docs = len(s)\n",
    "    s = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Symbolic ling for linux psql connection (No ideia if it is needed for windows)\n",
    "#Old snorkel-thingy\n",
    "if separator == '/':\n",
    "    os.system(\"ln -s /var/run/postgresql/.s.PGSQL.5432 /tmp/.s.PGSQL.5432\")\n",
    "\n",
    "#Adding the DB to the PATH\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "\n",
    "#SnorkelSession MUST ALWAYS BE after setting the os.environ, it will default to SQLite if the var is not set.\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5984 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5984/5984 [06:02<00:00, 16.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 5984\n",
      " \n",
      "Sentences: 8280\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import TSVDocPreprocessor\n",
    "\n",
    "doc_preprocessor = TSVDocPreprocessor(corpusPath, max_docs=n_docs)\n",
    "\n",
    "\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser import CorpusParser\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy(lang='pt'))\n",
    "\n",
    "\n",
    "#%time \n",
    "corpus_parser.apply(doc_preprocessor, count=n_docs, parallelism=16)\n",
    "\n",
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(' ')\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "relation = candidate_subclass('Relation', ['Drug', 'Event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Extractor and sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import DictionaryMatch\n",
    "\n",
    "ngrams = Ngrams(n_max=7)\n",
    "drug_matcher = DictionaryMatch(d=drugs, longest_match_only=True, ignore_case=True)\n",
    "event_matcher = DictionaryMatch(d=events, longest_match_only=True, ignore_case=True)\n",
    "cand_extractor = CandidateExtractor(relation, [ngrams, ngrams], [drug_matcher, event_matcher], symmetric_relations=True)\n",
    "\n",
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if i % 10 == 8:\n",
    "            dev_sents.add(s)\n",
    "        elif i % 10 == 9:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            train_sents.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the candidate extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6659 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.34 µs\n",
      "0\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6659/6659 [05:27<00:00, 20.34it/s]\n",
      "  0%|          | 0/810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 5156\n",
      "1\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:34<00:00, 23.44it/s] \n",
      "  0%|          | 0/811 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 652\n",
      "2\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 811/811 [00:41<00:00, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    print(i)\n",
    "    cand_extractor.apply(sents, split=i, parallelism = 16)\n",
    "    print(\"Number of candidates:\", session.query(relation).filter(relation.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import is_inverted\n",
    "\n",
    "def get_text(cand):\n",
    "    return cand.get_parent()._asdict().get('text')\n",
    "\n",
    "def get_spans_all(cand):\n",
    "    return [ \n",
    "                [\n",
    "                    cand.get_contexts()[0].get_span(), \n",
    "                    str(cand.get_contexts()[0].char_start), \n",
    "                    str(cand.get_contexts()[0].char_end), \n",
    "                    cand.get_contexts()[0].__str__().split(' ')[3][9:-1].replace(']', '').replace(',', '')\n",
    "                ],\n",
    "                [\n",
    "                    cand.get_contexts()[1].get_span(), \n",
    "                    str(cand.get_contexts()[1].char_start), \n",
    "                    str(cand.get_contexts()[1].char_end), \n",
    "                    cand.get_contexts()[1].__str__().split(' ')[3][9:-1].replace(']', '').replace(',', '')\n",
    "                ]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSVs for Snorkel 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 68/5156 [00:00<00:07, 673.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /home/lzirondi/Snorkel_Farmacovigilancia/Datasets/Preprocessed/train_sets.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5156/5156 [00:07<00:00, 657.54it/s]\n",
      " 10%|▉         | 65/652 [00:00<00:00, 646.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  /home/lzirondi/Snorkel_Farmacovigilancia/Datasets/Preprocessed/train_sets.tsv\n",
      "\n",
      " /home/lzirondi/Snorkel_Farmacovigilancia/Datasets/Preprocessed/dev_sets.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:00<00:00, 663.96it/s]\n",
      " 10%|█         | 63/626 [00:00<00:00, 629.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  /home/lzirondi/Snorkel_Farmacovigilancia/Datasets/Preprocessed/dev_sets.tsv\n",
      "\n",
      " /home/lzirondi/Snorkel_Farmacovigilancia/Datasets/Preprocessed/test_sets.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [00:00<00:00, 695.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  /home/lzirondi/Snorkel_Farmacovigilancia/Datasets/Preprocessed/test_sets.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "names = [\n",
    "    f'{outPath}{separator}Preprocessed{separator}train_sets.tsv', \n",
    "    f'{outPath}{separator}Preprocessed{separator}dev_sets.tsv', \n",
    "    f'{outPath}{separator}Preprocessed{separator}test_sets.tsv'\n",
    "    ]\n",
    "\n",
    "COLUMNS = \"Candidate\\tDrug\\tDrug_Char_Start\\tDrug_Char_end\\tDrug_Word_Index\\tEvent\\tEvent_Char_Start\\tEvent_Char_end\\tEvent_Word_Index\\tIs_inverted\\tLABEL\\n\"\n",
    "LABEL = '-1'\n",
    "\n",
    "for i in range(3):\n",
    "    with open(names[i], 'w') as f:\n",
    "        print('\\n', names[i])\n",
    "        f.write(COLUMNS)\n",
    "        for c in tqdm(session.query(relation).filter(relation.split == i).all()):\n",
    "            \n",
    "            candidate = get_text(c).replace('\\n', '')\n",
    "            drug, event = get_spans_all(c)\n",
    "            inverted = '1' if is_inverted(c) else '0'\n",
    "\n",
    "            f.write(\n",
    "                candidate + '\\t' +  \n",
    "                drug[0] + '\\t' + \n",
    "                drug[1] + '\\t' + \n",
    "                drug[2] + '\\t' + \n",
    "                drug[3] + '\\t' +  \n",
    "                event[0] + '\\t' + \n",
    "                event[1] + '\\t' + \n",
    "                event[2] + '\\t' + \n",
    "                event[3] + '\\t' +\n",
    "                inverted + '\\t' +\n",
    "                LABEL + '\\n'\n",
    "            )\n",
    "        print('Done ', names[i])\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "3bf8c17c33bc5a7faae2d0edfcd11d01c7f9c60d8b494ee9336faf390fb6541d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
