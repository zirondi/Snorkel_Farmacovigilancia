{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reexecução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "relation = candidate_subclass('Causa', ['Substância', 'Evento_Adverso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os arquivos dos dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário LIWC de Causas gerado com sucesso.\n",
      "Dicionário LIWC de Verbos gerado com sucesso.\n",
      "Dicionário LIWC de Causas & Verbos gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import Preprocessing as prep\n",
    "\n",
    "inPath = ''\n",
    "outPath = ''\n",
    "corpusPath = ''\n",
    "LIWCpath = '/home/lzirondi/Github/snorkel/Scripts/'\n",
    "\n",
    "util = prep.Util(inPath, outPath, LIWCpath, False)\n",
    "\n",
    "util.LIWKdicios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/home/lzirondi/Github/snorkel/Scripts/Dicts/Causas',\n",
    "         '/home/lzirondi/Github/snorkel/Scripts/Dicts/Verbos',\n",
    "         '/home/lzirondi/Github/snorkel/Scripts/Dicts/Causas & Verbos'\n",
    "]\n",
    "\n",
    "causas = {}\n",
    "verbos = {}\n",
    "causas_e_verbos = {}\n",
    "\n",
    "dicios = {\n",
    "    paths[0] : causas,\n",
    "    paths[1] : verbos,\n",
    "    paths[2] : causas_e_verbos\n",
    "}\n",
    "\n",
    "for file in paths:\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    s = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    for word in s:\n",
    "        word = word.replace('\\n', '')\n",
    "        dicios.get(file)[word] = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "dicios = [causas, verbos, causas_e_verbos]\n",
    "\n",
    "#chegar no candidato e ver se entre os tokens tem algo que a gente quer\n",
    "\n",
    "for candidate in cands:\n",
    "    for word in get_between_tokens(candidate):\n",
    "        if word not in sw:\n",
    "            print(word)\n",
    "            aux = False\n",
    "            if causas.get(word):\n",
    "                causas[word] = causas[word] + 1\n",
    "                print(word)\n",
    "                if verbos.get(word):\n",
    "                    aux = True\n",
    "                    verbos[word] = verbos[word] + 1\n",
    "                    causas_e_verbos[word] = causas_e_verbos[word] + 1\n",
    "\n",
    "            if(aux):\n",
    "                print(word)\n",
    "                verbos[word] = verbos[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "numero = re.compile('\\D+')\n",
    "repeticao = re.compile('\\w{2,}')\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "dic = {}\n",
    "\n",
    "#nao repetir frases\n",
    "#ignorar palavras de 1 letra \n",
    "\n",
    "for candidate in cands:\n",
    "    for word in get_between_tokens(candidate):\n",
    "        if word not in sw and len(word)>1 and numero.match(word):\n",
    "            if dic.get(word):\n",
    "                dic[word] = dic[word] + 1\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "                \n",
    "ordenada = []\n",
    "maior = ['', 0]\n",
    "\n",
    "dici = dic\n",
    "\n",
    "\n",
    "for i in range(0, len(dici)):\n",
    "    for key in dici:\n",
    "        if dici[key] > maior[1]:\n",
    "            maior[0] = key\n",
    "            maior[1] = dici[key]\n",
    "    ordenada.append(maior)\n",
    "    dici.pop(maior[0])\n",
    "    maior = ['', 0]\n",
    "                \n",
    "\n",
    "f = open('/home/lzirondi/Github/snorkel/Scripts/Análise de Palavras/between_tokens_stopFiltered.tsv', 'w', encoding='UTF-8')\n",
    "\n",
    "for word in ordenada:\n",
    "    f.write(word[0] + '\\t' + str(word[1]) + '\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "#print(session.query(relation).filter(relation.split == 1))\n",
    "\n",
    "\n",
    "#candi = cands[0]\n",
    "\n",
    "print(type(candi))\n",
    "print(cands[0].id)\n",
    "for candi in cands:\n",
    "    print(candi.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 5984\n",
      "Sentences: 6031\n",
      "6398\n",
      "6398\n",
      "6398\n",
      "3099::sentence:0:138\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "print(len(cands))\n",
    "\n",
    "rep = []\n",
    "\n",
    "for candidate in cands:\n",
    "    s = candidate.get_parent().stable_id.split(':')[0]\n",
    "    \n",
    "    if s not in rep:\n",
    "        rep.append(candidate.id)\n",
    "\n",
    "print(len(rep))\n",
    "print(len(cands))\n",
    "#print(dir(cands[0]))\n",
    "#print(cands[0].get_parent())\n",
    "\n",
    "print(cands[0].get_parent().stable_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Snorkel7)",
   "language": "python",
   "name": "snorkel7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
