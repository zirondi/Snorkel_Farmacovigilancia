{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reexecução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "relation = candidate_subclass('Causa', ['Substância', 'Evento_Adverso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando o Dicionário de Descrisções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/dic/LIWC2015_pt_desc_only.dic', 'r', encoding='utf-8-sig') as liwc_desc:\n",
    "    desc_txt = liwc_desc.readlines()\n",
    "\n",
    "desc_dic = dict()\n",
    "\n",
    "for line in desc_txt:\n",
    "    line = line.split('\\t')\n",
    "    \n",
    "    cat = line[1].replace('\\n', '')\n",
    "    cat = cat.split(' ')[1:]\n",
    "    aux = ' '\n",
    "    cat = ''.join(cat)\n",
    "    \n",
    "    cat = cat.replace('(', '')\n",
    "    cat = cat.replace(')', '')\n",
    "    \n",
    "    desc_dic[line[0]] = cat\n",
    "    \n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt_desc_only.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(desc_dic, json_file, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerado o Dicionário das Palavras do LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/dic/LIWC2015_pt_no_desc.dic', 'r', encoding='utf-8-sig') as liwc_words:\n",
    "    words_txt = liwc_words.readlines()\n",
    "\n",
    "words_dic = dict()\n",
    "\n",
    "for line in words_txt:\n",
    "    tags = []\n",
    "    line = line.split('\\t')\n",
    "    for element in line:\n",
    "        if element != '' and element != '\\n':\n",
    "            tags.append(element)\n",
    "    \n",
    "    words_dic[tags[0]] = tags[1:]\n",
    "    \n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt_no_desc.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(words_dic, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntado os dois dicionários (criando um dict do LIWC com as descrições ao invés dos números) ?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt_desc_only.json', 'r', encoding='utf-8') as desc_file:\n",
    "    desc_dict = json.load(desc_file)\n",
    "#print(desc_dict)    \n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt_no_desc.json', 'r', encoding='utf-8') as words_file:\n",
    "    words_dict = json.load(words_file)\n",
    "#print(words_dict)\n",
    "for key in words_dict:\n",
    "    tags = words_dict[key]\n",
    "    \n",
    "    for i in range(len(tags)):\n",
    "        #print(tags[i].replace('/n', ''))\n",
    "        tags[i] = desc_dict[tags[i].replace('\\n', '')]\n",
    "    \n",
    "    words_dict[key] = tags\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(words_dict, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "candidatos interessantes:\n",
    "\n",
    "split 0 - 100\n",
    "    Alergia\n",
    "split 0 - 200\n",
    "    Falta de pontuação prejudica o entendimento do tweet\n",
    "split 0 - 333-332\n",
    "    Exemplo simples\n",
    "split 0 - 838-853\n",
    "    Vários candidatos numa mesma sentença\n",
    "'''\n",
    "candidate = session.query(relation).filter(relation.split == 0).all()[332]\n",
    "print_candidate(candidate)\n",
    "print(get_stable_id(candidate))\n",
    "print(alt_get_stable_id(candidate))\n",
    "\n",
    "\n",
    "print(dir(candidate.get_contexts()[0]))\n",
    "print(candidate.get_contexts()[0].sentence_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text, get_text_splits,\n",
    "    is_inverted,\n",
    ")\n",
    "\n",
    "def get_text(cand):\n",
    "    return cand.get_parent()._asdict().get('text')\n",
    "\n",
    "def get_spans(cand):\n",
    "    return [cand.get_contexts()[0].get_span(), cand.get_contexts()[1].get_span()]\n",
    "\n",
    "def get_spans_complex(cand):\n",
    "    if(is_inverted(cand)):\n",
    "        return {0:cand.get_contexts()[1].get_span(), 1:cand.get_contexts()[0].get_span()}\n",
    "    else:\n",
    "        return {0:cand.get_contexts()[0].get_span(), 1:cand.get_contexts()[1].get_span()}\n",
    "\n",
    "def get_stable_id(cand):\n",
    "    return cand[0].stable_id.split(':')[0]\n",
    "\n",
    "def alt_get_stable_id(cand):\n",
    "    return cand[1].stable_id.split(':')[0]\n",
    "\n",
    "def highlight_spans(word_list, span_list):\n",
    "    s = ''\n",
    "    for word in word_list:\n",
    "        if word in span_list:\n",
    "            s += '*' + word + '* '\n",
    "        else:\n",
    "            s += word + ' '\n",
    "    return s\n",
    "\n",
    "def print_candidate(cand, switch=False):\n",
    "    if switch: print(\"Candidato: \" + str(cand), end='\\n')\n",
    "    print(\"Tweet: \" + highlight_spans(get_text(cand).split(' '), get_spans(cand)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#test stuff\n",
    "#ids = [100,200,332,333,838,839,840,853]\n",
    "\n",
    "candidates = (session.query(relation).filter(relation.split == 0).all() + \n",
    "    session.query(relation).filter(relation.split == 1).all() + \n",
    "    session.query(relation).filter(relation.split == 2).all())\n",
    "#candidates = []\n",
    "#for num in ids:\n",
    "#    candidates.append(session.query(relation).filter(relation.split == 0).all()[num])\n",
    "\n",
    "#for ca in candidates:\n",
    "#    print_candidate(ca)\n",
    "#/test stuff\n",
    "############################################################################################\n",
    "#Vars\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sets before_spans, between_spans and after_spans\n",
    "before_spans = []\n",
    "before_word_count = dict()\n",
    "#before_words\n",
    "#before_words_liwc\n",
    "    \n",
    "between_spans = []\n",
    "between_word_count = dict()\n",
    "#between_words\n",
    "#between_words_liwc\n",
    "\n",
    "after_spans = []\n",
    "after_word_count = dict()\n",
    "#after_words\n",
    "#after_words_liwc\n",
    "\n",
    "counts = [before_word_count, between_word_count, after_word_count]\n",
    "\n",
    "checked_ids = set()\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "for i in range(len(candidates)):\n",
    "#############################################################################################\n",
    "#Checking if the id was already processed\n",
    "    stable_id = get_stable_id(candidates[i])\n",
    "    #print(stable_id)\n",
    "    if stable_id not in checked_ids:\n",
    "        checked_ids.add(stable_id)\n",
    "        jump = 0\n",
    "        \n",
    "        while i+jump < len(candidates):\n",
    "            if stable_id == get_stable_id(candidates[i+jump]):\n",
    "                jump += 1\n",
    "            else: break\n",
    "        \n",
    "        candidate_list = []\n",
    "        if jump > 1:\n",
    "            for j in range(i, i+jump):\n",
    "                candidate_list.append(candidates[j])\n",
    "        else:\n",
    "            candidate_list.append(candidates[i])\n",
    "############################################################################################\n",
    "#Getting the candidates and tweet from the ID being checked\n",
    "        spans = []\n",
    "        \n",
    "        for c in candidate_list:\n",
    "            spans.append(get_spans_complex(c))\n",
    "        \n",
    "        tweet = get_text(candidate_list[0]).replace('\\n', '').split(' ')\n",
    "############################################################################################         \n",
    "#Removing the spans from the tweet and separating the sentence in before_spans, between_spans and after_spans\n",
    "        for dic in spans:\n",
    "            for key in dic:\n",
    "                for i in range(len(tweet)):\n",
    "                    if tweet[i] == dic[key]:\n",
    "                        tweet[i] = key\n",
    "        \n",
    "        last_zero = 0\n",
    "        first_one = 0\n",
    "        aux = True\n",
    "        for i in range(len(tweet)):\n",
    "            if type(tweet[i]) == int:\n",
    "                if tweet[i] == 0:\n",
    "                    last_zero = i\n",
    "                elif aux:\n",
    "                    first_one = i\n",
    "                    aux = False\n",
    "                       \n",
    "            \n",
    "        if first_one < last_zero:\n",
    "            first_one, last_zero = last_zero, first_one\n",
    "        \n",
    "        before_spans = tweet[:last_zero]\n",
    "        between_spans = tweet[last_zero+1:first_one]\n",
    "        after_spans = tweet[first_one+1:]\n",
    "                \n",
    "############################################################################################\n",
    "        categories_tweet = [before_spans, between_spans, after_spans]\n",
    "        for categories in categories_tweet:\n",
    "            if categories == before_spans:\n",
    "                aux = before_word_count\n",
    "            elif categories == between_spans:\n",
    "                aux = between_word_count\n",
    "            else:\n",
    "                aux = after_word_count\n",
    "            \n",
    "            for word in categories:\n",
    "                if type(word) != int and word in aux and word not in sw:\n",
    "                    aux[word] += 1\n",
    "                elif type(word) != int and word not in sw:\n",
    "                    aux[word] = 1\n",
    "\n",
    "############################################################################################\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/before_word_count.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(before_word_count, json_file, indent=2)\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/between_word_count.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(between_word_count, json_file, indent=2)\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/after_word_count.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(after_word_count, json_file, indent=2)\n",
    "############################################################################################        \n",
    "print('________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tom': [300, ['60', '62']], 'café*': [36, ['70', '74', '111']], 'forte': [12, ['1', '13', '21', '30', '31', '80', '83', '100', '102']], 'aparece': [2, ['20', '50', '54', '55', '60', '61', '91']], 'deu': [22, ['20', '40', '90']], 'mãe*': [40, ['40', '41', '43']], 'toda': [23, ['1', '2', '9', '40']], 'hm*': [18, ['120', '122', '124']], 'per': [2, ['1', '11']], 'senti': [12, ['20', '50', '51', '60', '63', '90']], 'acha': [2, ['20', '50', '51', '91']], 'cd*': [11, ['111']], 'pior': [28, ['1', '13', '21', '22', '30', '32', '80', '85']], 'podia': [6, ['1', '12', '20', '50', '54', '90']], 'tortur*': [1, ['30', '32', '34']], 'ser': [59, ['1', '12']], 'amante*': [1, ['40', '42', '70', '73', '80', '81']], 'pesquisa': [5, ['60', '61', '110']], 'dj*': [1, ['60', '62', '111']], 'ano': [23, ['100', '103']], 'risperidona': [1, ['70', '72']], 'mt*': [24, ['120', '122']], 'par': [1, ['25']], 'pública*': [1, ['40']], 'sob': [19, ['1', '11', '80', '83', '100', '102']], 'passa': [21, ['20', '91', '100', '101']], 'naquela': [2, ['1', '2', '9', '40', '43']], 'loja': [1, ['110', '113']], 'vou': [135, ['1', '12', '20', '92', '100', '101']]}\n",
      "29\n",
      "{'liguei': [2, None], 'tv': [1, None], 'onix': [1, None], 'lorenzoni': [1, None], 'bêbe': [1, None], 'nao': [31, None], '2': [54, None], 'gastrite': [1, None], 'brasileiros': [8, None], 'gente': [54, None], 'pq': [68, None], 'n': [51, None]}\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#Test stuff\n",
    "import json\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/before_word_count.json', 'r', encoding='utf-8') as json_file:\n",
    "    dic = json.load(json_file)\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "dic = dict(itertools.islice(dic.items(), 50))\n",
    "#/test stuff\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt_no_desc.json', 'r', encoding='utf-8') as json_file:\n",
    "    liwc = json.load(json_file)\n",
    "\n",
    "\n",
    "liwc_keys = list(liwc)    \n",
    "#removing emoticons from liwc\n",
    "for i in range(25):\n",
    "    del liwc[liwc_keys[i]]\n",
    "\n",
    "liwc_keys = list(liwc) \n",
    "on_liwc = dict()\n",
    "not_on_liwc = dict()\n",
    "\n",
    "a = set()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key in dic:\n",
    "    word = key\n",
    "    aux_bool = True\n",
    "    \n",
    "    for k in liwc_keys:\n",
    "        if len(k) < 3: \n",
    "            a.add(k)\n",
    "            pass\n",
    "        else:\n",
    "            regex = re.compile(k)\n",
    "            if regex.match(word):\n",
    "                #print(word, k)\n",
    "                aux_bool = False\n",
    "                on_liwc[k] = [dic[key], liwc[k]]\n",
    "                break\n",
    "    \n",
    "    if aux_bool:\n",
    "        not_on_liwc[key] = [dic[key], None]\n",
    "\n",
    "#print(len(a))\n",
    "#print(a)\n",
    "print(on_liwc)\n",
    "print(len(on_liwc))\n",
    "print(not_on_liwc)\n",
    "print(len(not_on_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t+': [194, ['120', '122']], 'ca': [1, ['1', '14']], 'forte': [12, ['1', '13', '21', '30', '31', '80', '83', '100', '102']], 'a': [1, ['1', '2', '3', '7', '9', '10', '11']], 'o': [3, ['1', '2', '3', '7', '9', '10']], 'lo': [1, ['1', '3', '2', '7']], 'de': [3, ['1', '11']], 'mãe*': [40, ['40', '41', '43']], 'hora': [18, ['100', '103']], 'per': [2, ['1', '11']], 'se': [55, ['1', '2', '3', '7', '9', '14', '23', '50', '53', '54', '56']], 'pior': [28, ['1', '13', '21', '22', '30', '32', '80', '85']], 'po': [6, ['120', '122', '125']], 'na': [2, ['1', '10', '11']], 'com': [11, ['1', '11']], 'pesquisa': [5, ['60', '61', '110']], 'do': [1, ['1', '10', '11']], 'risperidona': [1, ['70', '72']], 'mt*': [24, ['120', '122']], 'par': [3, ['25']], 'consulta': [2, ['50', '51']], 'pública*': [1, ['40']], 'sob': [19, ['1', '11', '80', '83', '100', '102']], 'passa': [22, ['20', '91', '100', '101']], 'vou': [135, ['1', '12', '20', '92', '100', '101']], 'pq': [68, ['50', '52', '120', '122']], 'legal': [5, ['1', '13', '21', '30', '31', '120', '123']], 'vem': [8, ['20', '91', '100', '101']], 'vi': [33, ['20', '60', '61', '90']], 'dá': [30, ['20', '40', '91']], 'vá': [6, ['1', '12']], 'golpe': [1, ['80', '83']], 'preciso': [30, ['1', '12', '20', '21', '50', '53', '55', '91']], 'e': [6, ['1', '14']], 'choro*': [4, ['30', '32', '35']], 'sabe': [14, ['20', '50', '51', '91', '120', '125']], 'faz': [37, ['20', '50', '52', '91']], 'pessoa': [29, ['40']], 'hoje': [57, ['1', '13', '91', '100', '103']], 'água': [7, ['70', '74']], 'q': [4, ['120', '122']], 'precisa': [4, ['1', '12', '20', '21', '50', '53', '55', '91']], 'dia': [89, ['100', '103']], 'cr': [1, ['80', '82', '110']], 'bem': [29, ['1', '2', '9', '13', '30', '31', '50', '54']], 'cheia': [11, ['100', '102']], 'saudade*': [7, ['30', '32', '35']], 'ia': [16, ['20', '90', '100', '101']], 'bar': [1, ['70', '74', '100', '102', '111']], 'pra': [298, ['1', '10', '11']], 'preocupação': [2, ['30', '32', '33', '80', '85']], 'plano': [2, ['20', '21', '91', '100', '101', '102']], 'primeiro': [9, ['1', '13', '24', '80', '82', '100', '103']]}\n",
      "{'liguei': [2, None], 'colica': [2, None], 'bêbe': [1, None], '2': [54, None], 'gastrite': [1, None], 'brasileiros': [8, None], 'gente': [54, None], 'n': [51, None], 'inventei': [3, None], 'ce': [3, None]}\n"
     ]
    }
   ],
   "source": [
    "#Test stuff\n",
    "import json\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/before_word_count.json', 'r', encoding='utf-8') as json_file:\n",
    "    dic = json.load(json_file)\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "dic = dict(itertools.islice(dic.items(), 50))\n",
    "#/test stuff\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/LIWC2015_pt_no_desc.json', 'r', encoding='utf-8') as json_file:\n",
    "    liwc = json.load(json_file)\n",
    "\n",
    "\n",
    "liwc_keys = list(liwc)    \n",
    "#removing emoticons from liwc\n",
    "for i in range(25):\n",
    "    del liwc[liwc_keys[i]]\n",
    "\n",
    "liwc_keys = list(liwc) \n",
    "on_liwc = dict()\n",
    "not_on_liwc = dict()\n",
    "\n",
    "a = set()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for key in dic:\n",
    "    word = key\n",
    "    aux_bool = True\n",
    "    \n",
    "    for k in liwc_keys:\n",
    "        if '*' in k:\n",
    "            r = '^' + k.replace('*','')\n",
    "        else:\n",
    "            r = '^' + k\n",
    "        #print(r)\n",
    "        regex = re.compile(r)\n",
    "        if regex.match(word):\n",
    "            aux_bool = False\n",
    "            on_liwc[k] = [dic[key], liwc[k]]\n",
    "            break\n",
    "    \n",
    "    if aux_bool:\n",
    "        not_on_liwc[key] = [dic[key], None]\n",
    "\n",
    "#print(len(a))\n",
    "#print(a)\n",
    "print(on_liwc)\n",
    "print(len(on_liwc))\n",
    "print(not_on_liwc)\n",
    "print(len(not_on_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "#for dic in count:\n",
    "#    aux_dic = []\n",
    "#    for key in dic:\n",
    "#        aux.append([key,quant[key]])\n",
    "   \n",
    "#    aux_dic = sorted(aux_dic, key=itemgetter(1,0))\n",
    "\n",
    "#aux_dic = []\n",
    "#for key in counts[0]:\n",
    "    aux_dic.append([key, counts[0][key]])\n",
    "\n",
    "#aux_dic = sorted(aux_dic, key=itemgetter(1,0))\n",
    "\n",
    "#with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/before_word_count_test.json', 'w', encoding='utf-8') as json_file:\n",
    "#    json.dump(aux_dic, json_file, indent=2)\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/before_word_count.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(before_word_count, json_file, indent=2)\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/between_word_count.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(between_word_count, json_file, indent=2)\n",
    "\n",
    "with open('/home/lzirondi/Github/snorkel/Scripts/LIWC/json/after_word_count.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(after_word_count, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cands = [session.query(relation).filter(relation.split == 0).all(), session.query(relation).filter(relation.split == 1).all(), session.query(relation).filter(relation.split == 2).all()]\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "quant = dict()\n",
    "\n",
    "for group in cands:\n",
    "    for candidate in group:\n",
    "        for word in get_between_tokens(candidate):\n",
    "            if word not in sw:\n",
    "                if word in quant:\n",
    "                    quant[word] = quant[word] + 1\n",
    "                else:\n",
    "                    quant[word] = 1\n",
    "    \n",
    "\n",
    "words = []\n",
    "\n",
    "for key in quant:\n",
    "    aux = [key,quant[key]]\n",
    "    words.append(aux)\n",
    "    \n",
    "from operator import itemgetter\n",
    "\n",
    "words = sorted(words, key=itemgetter(1,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NotOLD problema das palavras repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = []\n",
    "for i in range(838,853):\n",
    "    cands.append(session.query(relation).filter(relation.split == 0).all()[i])\n",
    "\n",
    "text = []\n",
    "for candi in cands:\n",
    "    text.append(get_text_splits(candi))\n",
    "#print(len(text)) 15\n",
    "#print(len(text[1])) 5\n",
    "\n",
    "\n",
    "print_candidate(cands[0])\n",
    "for i in range(0, len(text[0])):\n",
    "    print(i)\n",
    "    for j in range(0,len(text)):\n",
    "        print(text[j][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "dicios = [causas, verbos, causas_e_verbos]\n",
    "\n",
    "#chegar no candidato e ver se entre os tokens tem algo que a gente quer\n",
    "\n",
    "for candidate in cands:\n",
    "    for word in get_between_tokens(candidate):\n",
    "        if word not in sw:\n",
    "            print(word)\n",
    "            aux = False\n",
    "            if causas.get(word):\n",
    "                causas[word] = causas[word] + 1\n",
    "                print(word)\n",
    "                if verbos.get(word):\n",
    "                    aux = True\n",
    "                    verbos[word] = verbos[word] + 1\n",
    "                    causas_e_verbos[word] = causas_e_verbos[word] + 1\n",
    "\n",
    "            if(aux):\n",
    "                print(word)\n",
    "                verbos[word] = verbos[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "numero = re.compile('\\D+')\n",
    "repeticao = re.compile('\\w{2,}')\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "dic = {}\n",
    "\n",
    "#nao repetir frases\n",
    "#ignorar palavras de 1 letra \n",
    "\n",
    "for candidate in cands:\n",
    "    for word in get_between_tokens(candidate):\n",
    "        if word not in sw and len(word)>1 and numero.match(word):\n",
    "            if dic.get(word):\n",
    "                dic[word] = dic[word] + 1\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "                \n",
    "ordenada = []\n",
    "maior = ['', 0]\n",
    "\n",
    "dici = dic\n",
    "\n",
    "\n",
    "for i in range(0, len(dici)):\n",
    "    for key in dici:\n",
    "        if dici[key] > maior[1]:\n",
    "            maior[0] = key\n",
    "            maior[1] = dici[key]\n",
    "    ordenada.append(maior)\n",
    "    dici.pop(maior[0])\n",
    "    maior = ['', 0]\n",
    "                \n",
    "\n",
    "f = open('/home/lzirondi/Github/snorkel/Scripts/Análise de Palavras/between_tokens_stopFiltered.tsv', 'w', encoding='UTF-8')\n",
    "\n",
    "for word in ordenada:\n",
    "    f.write(word[0] + '\\t' + str(word[1]) + '\\n')\n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "#sorted_list = []\n",
    "#for key in copy_dic:\n",
    "#    sorted_list.append([key,copy_dic[key]])\n",
    "\n",
    "#from operator import itemgetter\n",
    "#sorted_list = sorted(sorted_list, key=itemgetter(1,0))\n",
    "\n",
    "#print(sorted_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())\n",
    "\n",
    "print(len(cands))\n",
    "\n",
    "rep = []\n",
    "\n",
    "for candidate in cands:\n",
    "    s = candidate.get_parent().stable_id.split(':')[0]\n",
    "    \n",
    "    if s not in rep:\n",
    "        rep.append(candidate.id)\n",
    "\n",
    "print(len(rep))\n",
    "print(len(cands))\n",
    "#print(dir(cands[0]))\n",
    "#print(cands[0].get_parent())\n",
    "\n",
    "print(cands[0].get_parent().stable_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(20):\n",
    "        if j == 10:\n",
    "            break\n",
    "        else: print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Snorkel7)",
   "language": "python",
   "name": "snorkel7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
