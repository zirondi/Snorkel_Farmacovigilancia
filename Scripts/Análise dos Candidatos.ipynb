{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reexecução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'postgres:///lzirondi'\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "relation = candidate_subclass('Causa', ['Substância', 'Evento_Adverso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os arquivos dos dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário LIWC de Causas gerado com sucesso.\n",
      "Dicionário LIWC de Verbos gerado com sucesso.\n",
      "Dicionário LIWC de Causas & Verbos gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import Preprocessing as prep\n",
    "\n",
    "inPath = None\n",
    "outPath = None\n",
    "corpusPath = None\n",
    "LIWCpath = '/home/lzirondi/Github/snorkel/Scripts/'\n",
    "\n",
    "util = prep.Util(inPath, outPath, LIWCpath, False)\n",
    "\n",
    "util.LIWKdicios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/home/lzirondi/Github/snorkel/Scripts/Dicts/Causas',\n",
    "         '/home/lzirondi/Github/snorkel/Scripts/Dicts/Verbos',\n",
    "         '/home/lzirondi/Github/snorkel/Scripts/Dicts/Causas & Verbos'\n",
    "]\n",
    "\n",
    "causas = {}\n",
    "verbos = {}\n",
    "causas_e_verbos = {}\n",
    "\n",
    "dicios = {\n",
    "    paths[0] : causas,\n",
    "    paths[1] : verbos,\n",
    "    paths[2] : causas_e_verbos\n",
    "}\n",
    "\n",
    "for file in paths:\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    s = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    for word in s:\n",
    "        word = word.replace('\\n', '')\n",
    "        dicios.get(file)[word] = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "dicios = [causas, verbos, causas_e_verbos]\n",
    "\n",
    "#chegar no candidato e ver se entre os tokens tem algo que a gente quer\n",
    "\n",
    "for candidate in cands:\n",
    "    for word in get_between_tokens(candidate):\n",
    "        if word not in sw:\n",
    "            print(word)\n",
    "            aux = False\n",
    "            if causas.get(word):\n",
    "                causas[word] = causas[word] + 1\n",
    "                print(word)\n",
    "                if verbos.get(word):\n",
    "                    aux = True\n",
    "                    verbos[word] = verbos[word] + 1\n",
    "                    causas_e_verbos[word] = causas_e_verbos[word] + 1\n",
    "\n",
    "            if(aux):\n",
    "                print(word)\n",
    "                verbos[word] = verbos[word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cands = session.query(relation).filter(relation.split == 0).all()\n",
    "\n",
    "sw = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for candidate in cands:\n",
    "    for word in get_between_tokens(candidate):\n",
    "        if word not in sw:\n",
    "            if dic.get(word):\n",
    "                dic[word] = dic[word] + 1\n",
    "                #criar\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "                \n",
    "\n",
    "f = open('/home/lzirondi/Github/snorkel/Scripts/Análise de Palavras/between_tokens_stopFiltered.tsv', 'w', encoding='UTF-8')\n",
    "\n",
    "for key in dic:\n",
    "    f.write(key + '\\t' + str(dic[key]) + '\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "novo\n"
     ]
    }
   ],
   "source": [
    "d ={\n",
    "    'test': 1\n",
    "}\n",
    "\n",
    "d['test'] = d['test'] + 1\n",
    "\n",
    "d['novo'] = 'test'\n",
    "\n",
    "for x in d:\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Snorkel7)",
   "language": "python",
   "name": "snorkel7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
